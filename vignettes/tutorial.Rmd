---
title: "Tutorial"
author: "Alexander Knowlton"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This is a package that holds several useful functions for statistical
inference and prediction, including a one-sample t test, a linear regression,
a random forest function for penguins data, and a cross-validation function.

To install and load the package from GitHub, use the following code:
```{r eval = FALSE}
devtools::install_github("Codax2000/mypackage")
library(mypackage)
```

```{r echo = FALSE}
library(mypackage)
```

## my_t.test
`my_t.test` is a function that performs a one-sample t test on a set of given
data. It accepts alternative hypotheses and a value for mu, the null parameter.
It returns a `list` with values of the p value, the alternative hypothesis,
the t statistic, and the degrees of freedom of the data.

For example, to test the hypotheses $$H_0: \mu=60$$ $$H_a: \mu \neq 60$$ on the
`lifeExp` column of the `my_gapminder` data, use the following code:
```{r}
local_gapminder = na.omit(my_gapminder)
result <- my_t.test(local_gapminder$lifeExp, alternative = "two.sided", mu = 60)
print(result)
```
Since the p-value of the data is `r round(result$p_val, 3)`, this test indicates
that, at $\alpha=0.05$, there is no evidence that the true mean
life expectancy of the gapminder data is not equal to 60.

To test the hypotheses $$H_0: \mu=60$$ $$H_a: \mu \lt 60$$ on the
`lifeExp` column of the `my_gapminder` data, use the following code:
```{r}
local_gapminder = na.omit(my_gapminder)
result <- my_t.test(local_gapminder$lifeExp, alternative = "less", mu = 60)
print(result)
```
Since the p-value of the data is `r round(result$p_val, 3)`, this test indicates
that, at $\alpha=0.05$, there is evidence that the true mean
life expectancy of the gapminder data is less than 60.

And finally, to test the hypotheses that $$H_0: \mu=60$$ $$H_a: \mu \gt 60$$
```{r}
local_gapminder = na.omit(my_gapminder)
result <- my_t.test(local_gapminder$lifeExp, alternative = "greater", mu = 60)
print(result)
```
Since the p-value of the data is `r round(result$p_val, 3)`, this test indicates
that, at $\alpha=0.05$, there is no evidence that the true mean
life expectancy of the gapminder data is greater than 60.

## my_lm
`my_lm` is a function that fits a linear model to a data set. It takes a data
set and a formula as a parameter and returns a
matrix with the coefficients of each predictor, the t statistic for each one,
the standard error of each coefficient, and the p value of each one, which
tells the significance of that coefficient.

For example, we
can fit a model to the `gapminder` data to try and predict life expectancy from
gdp per capita and the continent.

To fit the model, use the following:
``` {r}
local_gapminder <- na.omit(my_gapminder)
result <- my_lm(local_gapminder, lifeExp ~ gdpPercap + continent)
print(result)
```

the coefficient for `gdpPercap` is `r round(result[2, 1], 4)`. This indicates
that, if `continent` is kept constant, an increase of 1 in `gdpPercap` will
mean an increase of `r round(result[2, 1], 4)` years in life
expectancy.

The null and alternative hypotheses of the `gdpPercap` coefficient is:
$$H_0: \beta = 0$$
$$H_a: \beta \neq 0$$

The p-value is `r round(result[2, 4], 3)`. Such a small p-value indicates that
if the actual value of $\beta$ was 0, there would be a miniscule chance of
getting the observed $\beta$ by accident. Therefore, we can reject the null
hypothesis in favor of the alternative hypothesis, indicating that `gdpPercap`
is useful in the model.

This model can then be used for prediction. To do this, use the coefficients
from the result of `my_lm` to make predicted values. For example, we can do this
with the previous model, using `model.matrix` to get the predicted values,
given the coefficients from the model. We can also plot the fitted values
against the actual values to see how accurate the model is.

```{r}
beta <- result[, 1]
X <- model.matrix(lifeExp ~ gdpPercap + continent, local_gapminder)
fitted <- X %*% beta
tmp <- data.frame("Fitted" = fitted,  "Actual" = my_gapminder$lifeExp,
                  "Continent" = my_gapminder$continent)
ggplot2::ggplot(tmp, ggplot2::aes(x = Fitted, y = Actual, 
                                  color = Continent)) + ggplot2::geom_point()
```

This plot indicates that, depending on the continent, the model's accuracy
changes. For example, the actual and fitted values bear little resemblance
in Africa, but appear to be more linearly correlated in Europe.

## my_knn_cv
`my_knn_cv` takes four parameters: `train`, a data frame with the training
columns; `cl`, which represents the actual response variables; `k_nn`, the
number of nearest neighbors to use in the model; and `k_cv`, which is the
number of folds to use for cross-validation.

For example, we can try to predict the species of a penguin using the bill
length, bill depth, flipper length, and the body mass. We will use 5-fold
cross validation.

```{r}
library(class) ##include knn for my_knn_cv
library(randomForest)
local_penguins <- na.omit(my_penguins)
actual <- local_penguins$species
prediction_values <- local_penguins[c("bill_length_mm", "bill_depth_mm",
                                  "body_mass_g", "flipper_length_mm")]
results <- matrix(nrow = 10, ncol = 3)
colnames(results) <- c("Nearest Neighbors", "Training Error", "CV Misclassification")

for (i in 1:10) {
  result <- my_knn_cv(prediction_values, actual, i, 5)
  results[i, 1] <- i
  results[i, 2] <- mean(actual != result$class)
  results[i, 3] <- result$cv_error
}
as.table(results)
```
The function cross-validates the K-nearest neighbors model by splitting
the training data randomly into 5 folds, then trains the model on the data not
in a particular fold and testing on the data in the fold. This allows all of the
data to be used in training the model while also making sure there is still a
testing data set.

Since both the CV misclassification and the training error is lower with `knn=1`
, the best model to use would have 1 nearest neighbor.

## my_rf_cv
`my_rf_cv` is a function that takes in a parameter of the number of folds to
use in cross-validation. It then attempts to predict the body mass of penguins
from the bill length, bill depth, and flipper length.

We can examine the effect of the cv parameter thus:
```{r}
library(ggplot2)
k <- c(2, 5, 10)
result <- matrix(nrow = 90, ncol = 2)
colnames(result) <- c("n_folds", "cv_error")
for (i in 1:3) {
  for (j in 1:30) {
    result[(30) * (i - 1) + j, 1] <- k[i]
    result[(30) * (i - 1) + j, 2] <- my_rf_cv(k[i])
  }
}

result <- as.data.frame(result)
ggplot(result, aes(x=n_folds, y=cv_error, 
                   group = n_folds)) + geom_boxplot() + labs(title = 
                          "Random Forest with 5-fold Cross validation",
                          x = "Number of folds", y = "Mean Squared CV Error")
result_stats <- matrix(nrow = 3, ncol = 2)
colnames(result_stats) <- c("mean", "standard deviation")
rownames(result_stats) <- c("k = 2", "k = 5", "k = 10")
for (i in 1:3) {
  results_of_specific_k <- result[result$n_folds == k[i], 2]
  result_stats[i, 1] <- mean(results_of_specific_k)
  result_stats[i, 2] <- sd(results_of_specific_k)
}
result_stats <- (as.table(result_stats))
result_stats
```

It's clear that, as the number of folds increases, both the mean squared error
and the standard deviation of the squared error decrease. This makes sense,
since as there are more folds, the model has more training data to work with,
and can get a better grasp on the data itself. Therefore, its predictions
would be better. There are also fewer predictions, so the mean squared error
should be smaller with a smaller number of people as well.

